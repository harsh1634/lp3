import numpy as np  # For numerical operations
import matplotlib.pyplot as plt  # For plotting

# 1. Define the function we want to minimize (e.g., f(x) = x^2)
def f(x):
    return (x+3)**2  # Quadratic function

# 2. Define the gradient of the function (derivative)
def gradient(x):
    return 2 * x + 6  # Derivative of f(x) = x^2

# 3. Implement Gradient Descent
def gradient_descent(starting_point, learning_rate, num_iterations):
    x = starting_point  # Starting point
    x_history = [x]  # Store the history of x values for plotting

    for _ in range(num_iterations):
        x = x - learning_rate * gradient(x)  # Update rule
        x_history.append(x)  # Append the new x value to history

    return x, x_history  # Return the final x value and history

# 4. Parameters for Gradient Descent
starting_point = 2  # Starting point for the descent
learning_rate = 0.1  # Step size
num_iterations = 10000  # Number of iterations

# 5. Run Gradient Descent
final_x, x_history = gradient_descent(starting_point, learning_rate, num_iterations)

# 6. Print the results
print(f"Final x value: {final_x}")  # Print the final x value
print(f"Minimum value of f(x): {f(final_x)}")  # Print the minimum value of the function

# 7. Plotting the function and the descent path
x_values = np.linspace(-13, 7, 400)  # Create x values for plotting
plt.plot(x_values, f(x_values), label='f(x) = (x+3)^2')  # Plot the function
plt.scatter(x_history, f(np.array(x_history)), color='red', label='Descent Path')  # Plot the descent path
plt.title("Gradient Descent on f(x) = (x+3)^2")
plt.xlabel("x")
plt.ylabel("f(x)")
plt.axhline(0, color='black',linewidth=0.5, ls='--')  # Add x-axis
plt.axvline(0, color='black',linewidth=0.5, ls='--')  # Add y-axis
plt.legend()
plt.grid()
plt.show()  # Show the plot


#Theory
Gradient descent:- (optimization algo)
used to find minimum of cost function (eg: Mean Squared Error)
cost function tells us how far our predictions are from actual values
we want to minimize cost function to get best predictions

types of gradient descent:-
1) BATCH : sums entries for each point in training set, update model only after *all* training examples have been evaluated
2) STOCHASTIC : evaluates each training example one at a time instead of in a batch
3) MINI BATCH : splits training dataset into small batch sizes and performs updates on each of those

Disadvantages:-
1) may not find global minimum